{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlydNN4HBcJT"
      },
      "source": [
        "<center><img src=\"https://raw.githubusercontent.com/bazylip/gradient-live-session/main/lab3/img/logo.png\" width=100 heigth=100></center>\n",
        "\n",
        "<center><h1><b>Introduction to CNNs</b></h1></center>\n",
        "<center><h2><b>Gradient PG, 2021</b></h2></center>\n",
        "<center><h4><b>Marcin Walkowski</b></h4></center>\n",
        "\n",
        "---\n",
        "<img src=\"https://raw.githubusercontent.com/bazylip/gradient-live-session/main/lab4/img/colab.png\" width=30%>\n",
        "\n",
        "<a href=\"https://githubtocolab.com/Gradient-PG/gradient-live-session/blob/main/2021%20Edition/4.%20CNN/introduction_to_cnns.ipynb\">Run in Google Colab</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeeVdlhoBy-y"
      },
      "source": [
        "# Fashion MNIST Classifier\n",
        "\n",
        "In this notebook, we will return to the problem of image classification. This time, however, we will use the popular Fashion MNIST dataset, which consists of photos of clothing items. We will try an approach based on the architecture typical for computer vision problems. Using the Keras library, we will build simple, convolutional neural networks based classifier. In this notebook, we will use the knowledge gained from the <a href=\"https://colab.research.google.com/github/bazylip/gradient-live-session/blob/main/lab3/introduction_to_deep_learning.ipynb\">previous hands-on</a>.\n",
        "\n",
        "Remember to complete all sections marked with #TODO!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82WkFLABCDEw"
      },
      "source": [
        "# 1. Prepare the environment\n",
        "\n",
        "Before you get to work, go to ```Runtime``` → ```Change runtime type``` tab and select ```GPU```. Hardware acceleration will speed up code execution. This is particularly important in computer vision problems, where data are images and deep architectures are used.\n",
        "\n",
        "If you want to start the whole notebook from scratch, press ```Ctrl+F9```. Pressing ```Ctrl+Enter``` will only run the cell in which the cursor is currently located.\n",
        "\n",
        "Let's start by downloading the packages we need. Like last time, we will use Keras to build our classifier model. We will use NumPy for mathematical operations, and the PyPlot package for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65FTue07CRB3"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvQY5kI5CTJJ"
      },
      "source": [
        "#2. Explore the dataset\n",
        "\n",
        "The Fashion MNIST dataset comes with the Keras library. The dataset consists of pictures of clothing items and labels in the form of numbers representing specific categories of clothing. Keras provides a dataset already split into training and test subsets.\n",
        "\n",
        "**Reminder: The training set is used only to train our model. The test set is intended for the evaluation of the obtained model. It cannot be used for training!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTCUn5tDC33H"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMxFrtl7PEbq"
      },
      "outputs": [],
      "source": [
        "class_names = [\n",
        "    'T-shirt/top',\n",
        "    'Trouser',\n",
        "    'Pullover',\n",
        "    'Dress',\n",
        "    'Coat',\n",
        "    'Sandal',\n",
        "    'Shirt',\n",
        "    'Sneaker',\n",
        "    'Bag',\n",
        "    'Ankle boot'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IiHFYEhReth"
      },
      "source": [
        "Let's begin with some data exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gReClt0PRoIH"
      },
      "outputs": [],
      "source": [
        "print(f\"Train dataset shape: {train_images.shape}\")\n",
        "print(f\"Test dataset shape: {test_images.shape}\")\n",
        "print(f\"Minimal value: {np.min(train_images)}\")\n",
        "print(f\"Maximal value: {np.max(train_images)}\")\n",
        "print(\"-\"*20)\n",
        "print(f\"Train labels: {train_labels}\")\n",
        "print(f\"Test labels: {test_labels}\")\n",
        "print(f\"Train examples: {len(train_labels)}\")\n",
        "print(f\"Test examples: {len(test_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAjQwSAcTQeO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGf6xrdWUUc2"
      },
      "source": [
        "As you can see, the data structure is similar to the original MNIST dataset. The images are represented as 28x28 monochrome bitmaps, and the labels are numbers between 0 and 9.\n",
        "\n",
        "\n",
        "Like the last time, we normalize the pixel values ​​to the range from 0.0 to 1.0 and write them as float32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YD2U4GQUu1N"
      },
      "outputs": [],
      "source": [
        "train_images = #TODO normalize train images\n",
        "test_images = #TODO normalize test images\n",
        "\n",
        "print(f\"New minimal value: {np.min(train_images)}\")\n",
        "print(f\"New maximal value: {np.max(train_images)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl8FJ-zzSVwQ"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[0], cmap=plt.cm.binary)\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.title(class_names[train_labels[0]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nzv262aUxza"
      },
      "source": [
        "To pass the images as input to the convolutional layer, we need to extend them with an additional dimension representing depth. This is because the implementation of convolutional layers in Keras is adapted to processing, more complex data, like colour images (RGB), where the pixel is encoded with three values. The images from the Fashion MNIST set are monochromatic, so the number of channels will be one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i64hOltIqp5p"
      },
      "outputs": [],
      "source": [
        "train_images = np.expand_dims(train_images, -1)\n",
        "test_images = np.expand_dims(test_images, -1)\n",
        "\n",
        "print(f\"New train image shape: {train_images[0].shape}\")\n",
        "print(f\"New test image shape: {test_images[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5doMFT5trfXO"
      },
      "source": [
        "The size one extra dimension is often called the dummy dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQagNU2RDNpQ"
      },
      "source": [
        "# 3. Create the network\n",
        "Our model will consist of two parts. The first part is responsible for creating the image representation vector using the convolution operation. The second part is a dense classifier, which task is to predict the label for the obtained representation.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bazylip/gradient-live-session/main/lab4/img/nn.jpg\">\n",
        "\n",
        "We will use the ```keras.Sequential``` type to build our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2FZwZXq6ZH2"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "input_image_shape = (28, 28, 1)\n",
        "model.add(keras.Input(shape=input_image_shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJUITijP2SJY"
      },
      "source": [
        "We explicitly define the size of the input data by adding the ```keras.Input()``` object to the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqTK4CGWDWkv"
      },
      "source": [
        "## 3.1 Convolution layer\n",
        "\n",
        "The convolution operation applies a filter/kernel to the input data. A kernel multiplies the input data area by the learned weights and then sums them up. The purpose of the operation is to extract information about high-level features, such as edges, from the image. By shifting the kernel by some step, we create a new data representation containing information about high-level features. By applying many different filters we can create multiple feature maps of the input image.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bazylip/gradient-live-session/main/lab4/img/conv.gif\">\n",
        "\n",
        "<i>Image source: <a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\">A Comprehensive Guide to Convolutional Neural Networks</a></i>\n",
        "\n",
        "Keras provides the implementation of the convolutional layer. Our task is to provide the correct parameters. We will use 32 filters of 3x3 size. As the default filter step is one, the operation will produce 32 feature maps of the input data with a size of 26x26 each. We will use the ReLU activation function. It is currently the most popular activation function used in convolutional networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL0_klC08XQf"
      },
      "outputs": [],
      "source": [
        "conv_layer_1 = keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\")\n",
        "\n",
        "model.add(conv_layer_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2dycrcZD2fq"
      },
      "source": [
        "## 3.2 Pooling layer\n",
        "\n",
        "The purpose of the pooling layer is to reduce the spatial size of data. We want to reduce the computation time and extract only the dominant image features. Similarly to the convolution operation, a filter/kernel is shifted over the data. For Max-Pooling, the highest activation is selected from the kernel area. For Average-Pooling, we use the average activation value.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/bazylip/gradient-live-session/main/lab4\\/img/pool.gif\">\n",
        "\n",
        "<i>Image source: <a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\">A Comprehensive Guide to Convolutional Neural Networks</a></i>\n",
        "\n",
        "We will use the Max-Pooling operation available in Keras. We set the kernel size to 2x2. The default step is equal to the kernel width/height. The feature map size will be reduced from 26x26 to 13x13. Remembering that we are operating on 32 feature maps, the shape of the output data will be 13x13x32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kB2YAmb8w27"
      },
      "outputs": [],
      "source": [
        "pool_layer_1 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "model.add(pool_layer_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh6haosT9M-W"
      },
      "source": [
        "Now let's add an additional convolution and pooling layer to our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qqa39k7j9YS9"
      },
      "outputs": [],
      "source": [
        "#TODO Add Conv2D layer (I suggest 3x3 kernel and 64 filters, but fill free to experiment)\n",
        "#TODO Add MaxPool2D layer (I suggest 2x2 kernel)\n",
        "\n",
        "# model.add(conv_layer_2)\n",
        "# model.add(pool_layer_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNw4y01uD_bM"
      },
      "source": [
        "## 3.3 Dense classifier\n",
        "\n",
        "After convolutional operations, we obtained a new high-level representation of the data. We will use the fully connected network to classify it. To pass processed data to the dense layer, we have to transform it into a vector. Operation Flatten available in Keras \"flattens\" the matrix to one dimension.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWFaFvdH92C5"
      },
      "outputs": [],
      "source": [
        "model.add(keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgpoLuTgwSBf"
      },
      "source": [
        "\n",
        "The Dense layer should consist of 10 nodes and use the Softmax activation function so that the output on the nth node corresponds to the probability of the nth class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQyyBsIMwSR0"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "#TODO Add Dense layer\n",
        "# model.add(dense_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYePr8Zz_6og"
      },
      "source": [
        "Our simple classifier model is now ready! As you may have noticed, even building more complex architectures with ML libraries can be fast. For a better understanding of CNN's architecture, I recommend reading thru the article <a href=\"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\">\"A Comprehensive Guide to Convolutional Neural Networks\"</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdXlTnADAFMd"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ1LBtgJFlDS"
      },
      "source": [
        "# 4. Train the model\n",
        "\n",
        "To speed up the training process we will use the ```model.compile()``` method. As we are dealing with a classification problem, we will use the Categorical Crossentropy loss function. Due to the format of our labels, which are encoded as integers, we will use the ```keras.losses.sparse_categorical_crossentropy``` implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OovL01mAnX3"
      },
      "outputs": [],
      "source": [
        "loss_fn = #TODO Create loss function\n",
        "optimizer = #TODO Pick optimizer (Adam is a common choice)\n",
        "\n",
        "model.compile(loss=loss_fn, optimizer=optimizer, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiQnDssY1x1B"
      },
      "source": [
        "It's time to train our model! Adjust the number of epochs and the mini-batch size (15 epochs and batch size 128 is a good starting point)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfQP77uWA0hd"
      },
      "outputs": [],
      "source": [
        "batch_size = #TODO \n",
        "epochs = #TODO\n",
        "model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9GtjNlo2dO-"
      },
      "source": [
        "Finally, let's use the ```model.evaluate()``` method to see how our network deals with the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNDF0PtxDMGW"
      },
      "outputs": [],
      "source": [
        "score = #TODO Evalutae model on the test set\n",
        "print(f\"Test loss: {score[0]}\")\n",
        "print(f\"Test accuracy: {score[1]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9utgT1f2usZ"
      },
      "source": [
        "If you have completed all the #TODO sections correctly, you should get an accuracy of around 90% on the test set. It is a good result but leaves room for improvement. Try different network parameter configurations and see if you can improve this score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkbDqdN5RlQl"
      },
      "source": [
        "# 5. Use the trained model\n",
        "\n",
        "Use the code below to see how your trained model works. Each time you execute the cell below, the image is sampled from the test set, and the model predicts the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUMmQOZ94dCR"
      },
      "outputs": [],
      "source": [
        "# Sample image index\n",
        "index = np.random.randint(0, len(test_images))\n",
        "img = np.expand_dims(test_images[index], 0)\n",
        "# Predict label\n",
        "predictions_array = model.predict(img).squeeze()\n",
        "predicted_label = np.argmax(predictions_array)\n",
        "true_label = test_labels[index]\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "# Plot image\n",
        "plt.subplot(1,2,1)\n",
        "plt.grid(False)\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "plt.imshow(img.squeeze(), cmap=plt.cm.binary)\n",
        "if predicted_label == true_label:\n",
        "  color = 'blue'\n",
        "else:\n",
        "  color = 'red'\n",
        "\n",
        "plt.xlabel(\n",
        "    f\"{class_names[predicted_label]} \" + \n",
        "    f\"{100*np.max(predictions_array):2.0f}% \" +\n",
        "    f\"({class_names[true_label]})\",\n",
        "    color=color\n",
        "    )\n",
        "\n",
        "# Plot probabilities\n",
        "plt.subplot(1,2,2)\n",
        "plt.grid(False)\n",
        "plt.xticks(range(10), class_names, rotation=45)\n",
        "plt.yticks([])\n",
        "thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "plt.ylim([0, 1])\n",
        "predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "thisplot[predicted_label].set_color('red')\n",
        "thisplot[true_label].set_color('blue')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYq2MeK-FwsC"
      },
      "source": [
        "# 6. Experiment... with more complex data!\n",
        "\n",
        "Now that you know how to quickly build an image classification model try your hand with more complex data. The CIFAR10 dataset, available in Keras, contains pictures representing one of the ten types of objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o3iS2pXPpiT"
      },
      "outputs": [],
      "source": [
        "(cifar_train_images, cifar_train_labels), (cifar_test_images, cifar_test_labels) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "cifar_class_names = [\n",
        "    'Airplane',\n",
        "    'Automobile',\n",
        "    'Bird',\n",
        "    'Cat',\n",
        "    'Deer',\n",
        "    'Dog',\n",
        "    'Frog',\n",
        "    'Horse',\n",
        "    'Ship',\n",
        "    'Truck'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhuRPtPmAHYK"
      },
      "source": [
        "The dataset format is similar to Fashion MNIST, except that the images are 32x32 and are in colour (pixel encoded with three values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYsow1SuP9fj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(cifar_train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(cifar_class_names[cifar_train_labels[i][0]])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Introduction to CNNs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
