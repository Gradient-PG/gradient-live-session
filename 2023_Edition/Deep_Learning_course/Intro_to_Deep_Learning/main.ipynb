{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to deep learning with pytorch\n",
    "\n",
    "## Sources:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) # For reproducibility of results\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "## Dataset view: https://www.kaggle.com/datasets/zalando-research/fashionmnist\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "\n",
    "# We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels.\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x123574a10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying sample the data using matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib inline\n",
    "sample_data = training_data[16]\n",
    "image = sample_data[0].numpy()[0]\n",
    "print(f\"Image shape {image.shape}\")\n",
    "plt.imshow(image * 255, cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten() # convert each 2D 28x28 image into a contiguous array of 784 pixel values (the minibatch dimension (at dim=0) is maintained).\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512), # images are 28x28 pixels\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Ankle boot, Actual class: Trouser\n"
     ]
    }
   ],
   "source": [
    "# Forward propagation on first data sample\n",
    "\n",
    "X = sample_data[0].to(device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1).cpu().item()\n",
    "\n",
    "print(f\"Predicted class: {datasets.FashionMNIST.classes[y_pred]}, Actual class: {datasets.FashionMNIST.classes[sample_data[1]]}\")\n",
    "\n",
    "# Why wrong prediction? (depends on random initial weights) -> We havent trained our model yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # More about loss functions: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) # More about optimizers: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() # set model to training mode\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() # reset gradients\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # set model to evaluation mode\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 8.9%, Avg loss: 2.304297 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.314872  [   64/60000]\n",
      "loss: 0.555549  [ 6464/60000]\n",
      "loss: 0.385013  [12864/60000]\n",
      "loss: 0.497228  [19264/60000]\n",
      "loss: 0.427932  [25664/60000]\n",
      "loss: 0.448676  [32064/60000]\n",
      "loss: 0.364214  [38464/60000]\n",
      "loss: 0.523146  [44864/60000]\n",
      "loss: 0.470050  [51264/60000]\n",
      "loss: 0.540722  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.420927 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.263710  [   64/60000]\n",
      "loss: 0.368335  [ 6464/60000]\n",
      "loss: 0.279124  [12864/60000]\n",
      "loss: 0.384675  [19264/60000]\n",
      "loss: 0.406725  [25664/60000]\n",
      "loss: 0.424508  [32064/60000]\n",
      "loss: 0.303943  [38464/60000]\n",
      "loss: 0.490382  [44864/60000]\n",
      "loss: 0.412547  [51264/60000]\n",
      "loss: 0.454745  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.396090 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.217470  [   64/60000]\n",
      "loss: 0.339472  [ 6464/60000]\n",
      "loss: 0.233601  [12864/60000]\n",
      "loss: 0.326719  [19264/60000]\n",
      "loss: 0.404295  [25664/60000]\n",
      "loss: 0.359180  [32064/60000]\n",
      "loss: 0.276142  [38464/60000]\n",
      "loss: 0.434472  [44864/60000]\n",
      "loss: 0.332469  [51264/60000]\n",
      "loss: 0.402681  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.380580 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.221186  [   64/60000]\n",
      "loss: 0.299829  [ 6464/60000]\n",
      "loss: 0.210987  [12864/60000]\n",
      "loss: 0.264386  [19264/60000]\n",
      "loss: 0.346169  [25664/60000]\n",
      "loss: 0.315576  [32064/60000]\n",
      "loss: 0.226186  [38464/60000]\n",
      "loss: 0.423226  [44864/60000]\n",
      "loss: 0.292712  [51264/60000]\n",
      "loss: 0.362410  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.355349 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.188006  [   64/60000]\n",
      "loss: 0.248344  [ 6464/60000]\n",
      "loss: 0.221260  [12864/60000]\n",
      "loss: 0.246449  [19264/60000]\n",
      "loss: 0.364733  [25664/60000]\n",
      "loss: 0.299162  [32064/60000]\n",
      "loss: 0.228872  [38464/60000]\n",
      "loss: 0.357628  [44864/60000]\n",
      "loss: 0.284954  [51264/60000]\n",
      "loss: 0.307330  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.340698 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Initial accuracy\n",
    "test(test_dataloader, model, loss_fn)\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Trouser, Actual class: Trouser\n"
     ]
    }
   ],
   "source": [
    "# Forward propagation on first data sample\n",
    "\n",
    "X = sample_data[0].to(device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1).cpu().item()\n",
    "\n",
    "print(f\"Predicted class: {datasets.FashionMNIST.classes[y_pred]}, Actual class: {datasets.FashionMNIST.classes[sample_data[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
