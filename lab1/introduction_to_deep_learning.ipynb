{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "TODO: Introduction to Deep Learning.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "TPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lx6_sK4Ihs5y",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/bazylip/gradient-live-session/main/lab1/img/logo.png\" width=100 heigth=100></center>\n",
    "\n",
    "<center><h1><b>Introduction to Deep Learning</b></h1></center>\n",
    "<center><h2><b>Gradient PG, 2020</b></h2></center>\n",
    "<center><h4><b>Bazyli Polednia</b></h4></center>\n",
    "\n",
    "---\n",
    "<center><img src=\"https://raw.githubusercontent.com/bazylip/gradient-live-session/main/lab1/img/colab.png\" width=30%></center>\n",
    "\n",
    "<center><a href=\"https://colab.research.google.com/github/bazylip/gradient-live-session/blob/main/lab1/introduction_to_deep_learning.ipynb\">Run in Google Colab</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnqZcoDTkTzh"
   },
   "source": [
    "# MNIST Digit Classifier\n",
    "\n",
    "In this notebook you will get to know the very basics of Tensorflow library and learn how to use it to build a basic network classifying handwritten digits. It is a well-known task, sometimes considered as a \"Hello world\" problem in AI field. We will mostly use Keras - it is a high-level API coming as submodule of Tensorflow. I highly encourage you to explore more stuff from those libraries - we will deal only with a small part of what they are capable of.\n",
    "\n",
    "Apart from text sections below, you will also find pre-prepared snippets of code, some of them with \\#TODO sections - fill them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWbYezRWoWCb"
   },
   "source": [
    "# 1. Prepare the environment\n",
    "\n",
    "Before you start executing the code, change runtime time of this notebook. Go to \"Runtime\" â†’ \"Change runtime type\" and select \"TPU\". This will enable you to run the notebook faster using the hardware accelerator. If you want to run the whole notebook from the beginning, press Ctrl+F9. If you only want to rerun a certain cell, press Ctrl+Enter while your cursor is in the cell.\n",
    "\n",
    "Afterwards, let's install necessary libraries which we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kiUUhDClhlKq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "86c64710-d26e-460b-9deb-4f0c8734c455"
   },
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "!pip install mitdeeplearning\n",
    "import mitdeeplearning as mdl"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLIgBBVnqRob"
   },
   "source": [
    "#2. Explore the dataset\n",
    "\n",
    "Our task is to classify handwritten digits from MNIST dataset provided in Keras library. We will import it dividing the images and labels into train set and test set. Train set is the bunch of data we will be using to train the network on, meanwhile test set will be used to evaluate how well our network performs. It's important to remember that **you can't use test set to train the network** - it should only be used to evaluate the permorfance. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7XStzSPyrdmn"
   },
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M__yke0Vs-LH"
   },
   "source": [
    "Remember - the first thing you should always do before jumping straight to designing neural networks is explore the data you are using. Let's start with seeing in what format it is stored and how many images there are."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZ2f81k_tF2W",
    "outputId": "a898bbd1-5fae-42a2-9cb8-17c3d14ea01f"
   },
   "source": [
    "print(f\"Train images shape: {train_images.shape}\")\n",
    "print(f\"Test images shape: {test_images.shape}\")\n",
    "print(f\"Minimal value: {np.min(train_images)}\")\n",
    "print(f\"Maximal value: {np.max(train_images)}\")\n",
    "print(\"-\"*20)\n",
    "print(f\"Train labels: {train_labels}\")\n",
    "print(f\"Test labels: {test_labels}\")\n",
    "print(f\"Train labels length: {len(train_labels)}\")\n",
    "print(f\"Test labels length: {len(test_labels)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "6katIjV7tSqM",
    "outputId": "8d164f6d-bfa7-443f-ff6b-c5bcd3b264a3"
   },
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uc6gl8BhuMil"
   },
   "source": [
    "As you can see, each image is represented as 28x28 pixel map, where each pixel takes value from 0.0 to 255.0. Label is simply the digit presented on the corresponding image. \n",
    "\n",
    "In order to easily pass the image to the network, let's scale the values in each image to range from 0.0 to 1.0 and store them as *float32* type."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oCw1pXCdwREp"
   },
   "source": [
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "qBEbQwWcwkvH",
    "outputId": "5276a5d7-3f6f-4b29-8d84-2ad900c278f4"
   },
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(train_labels[i])\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5VvIvadydz7"
   },
   "source": [
    "#3. Create the network\n",
    "\n",
    "After we've done some very basic preprocessing and data exploration, let's get to the coolest thing - creating the network! There is no universal rule of thumb when it comes to which network architecture is best for a given problem. Picking the right one is more an art than a science and you can only get needed intuition by solving tens, hundreds and thousands of deep learning problems.\n",
    "\n",
    "This time we'll go with a very basic network design:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bazylip/gradient-live-session/main/lab1/img/graph.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yu6Xc8pwfo60"
   },
   "source": [
    "Let's take a moment to analyse what we are going to do. First, we create an empty network model. We use \"Sequential\" type - it means that all layers of the network will be connected one after another. In the next line, we add our first layer of newly created network. It is a dense layer taking a few arguments - number of nodes (512), activation function (ReLU) and shape of the input it receives - here it is a 1D vector of length 784 (28*28), so each input corresponds to a single pixel. Now go ahead and add the second dense layer! It should have 10 nodes and use softmax as activation function - this way n-th node will output the probability of input being \"n\" e.g. 4th node being equal to 0.4 will mean that there is a 40% that the digit on the image is a \"4\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tt_fC15d1kkM"
   },
   "source": [
    "def create_model():\n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(tf.keras.layers.Dense(512, \n",
    "                                  activation=\"relu\", \n",
    "                                  input_shape=(28*28,)))\n",
    "  #TODO: Add next layer\n",
    "  \n",
    "  return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3e4LnBGibiY"
   },
   "source": [
    "#4. Train the network\n",
    "\n",
    "Good job, it looks like we have our network ready! Let's print a summary of it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSrEEPlGfnK6",
    "outputId": "daf93251-33c7-45cb-b3f7-eb55edb00a5c"
   },
   "source": [
    "print(create_model().summary())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Afi2IFQitv6"
   },
   "source": [
    "In this case, \"params\" mean actual weigth parameters that we will be optimizing during network training. 407,050 parameters for which we have to calculate the gradient - that's quite a lot... Good thing Tensorflow already has it implemented! Let's learn some basics of automatic differentiation before applying it to our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nflzDtY0W0c_"
   },
   "source": [
    "##4.1 How to use tf.GradientTape()\n",
    "\n",
    "GradientTape is one of the core tools in Tensorflow. It allows you to record operations happening during automatic differentiation. Let's compute the gradient for function $y = x^2$ at $x=3$. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aodbuLoLXzrZ",
    "outputId": "b057e922-0721-4de5-b051-0bdf56419eac"
   },
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x * x\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "print(f\"dy/dx at x=3.0: {dy_dx}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bare03ErYS_K"
   },
   "source": [
    "It's super easy, isn't it? Now try to compute multiple gradient - we have two functions: $y = x^2$ and $z = y^2$, our goal is to compute $\\frac{\\partial z}{\\partial x}$ at $x=3$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_iTZZgjkZHyw",
    "outputId": "b2b03597-6696-41ad-af78-19d97a7b2c28"
   },
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  #TODO: Create y and z functions\n",
    "dz_dx = #TODO: Calculate the gradient\n",
    "print(f\"dz/dx at x=3.0: {dz_dx}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfU4lmvOZdmz"
   },
   "source": [
    "Let's check: $\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} * \\frac{\\partial y}{\\partial x} = 2y(x) \\cdot 2x = 4 \\cdot x^3 \\vert ^{x=3} = 4 \\cdot 27 = 108$\n",
    "\n",
    "Now you see how easy it is to compute the gradients using Tensorflow - you just tell which gradient you want to get and the library does everything for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dM4aJ6BfeFhx"
   },
   "source": [
    "##4.2 Train the network using GradientTape()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzUfnn-nfgE9"
   },
   "source": [
    "Because we engineered our network to take a 1D vector as an input, we have to convert our 28x28 images to match this format. Next, we will create a dataset using `tf.data.Dataset.from_tensor_slices()` - this function divides our images and labels using their first dimension (in our case this dimension is 60000 - dataset size). This way we will have a dataset in which every element will be a 1D vector of size 784 (image) with a label corresponding to it.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pSfG94z3shti"
   },
   "source": [
    "original_images = {\"train\": train_images, \"test\": test_images}\n",
    "\n",
    "train_images = train_images.reshape(60000, 28 * 28)\n",
    "test_images = test_images.reshape(10000, 28 * 28)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\"\"\"TODO: Pass the data\"\"\"))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jix26WAegsHD"
   },
   "source": [
    "Theoretically, we could say that our dataset is now ready - but there is one thing we could do to improve training performance. Instead of passing the whole dataset to the network at each iteration, we could split it into smaller parts called batches which we will pass to the model one at a time. This approach, called mini-batch gradient descent, will improve efficiency and enable faster convergence. Actually, it's quite a complicated concept - I encourage you to Google it later!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qz3AVneyjasR"
   },
   "source": [
    "batch_size = 256\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yVH3vKrjh1N"
   },
   "source": [
    "When the network is trained, it needs a reliable metric to know if it's doing better or worse - it's called the loss function. In our example, we'll be using `tf.keras.losses.sparse_categorical_crossentropy` function, which is very commonly used when dealing with classification models with probabilities as outputs. It uses standard cross entropy formula:\n",
    "\n",
    "$J(W)=\\frac{1}{n} \\cdot \\sum_{i=1}^{n} y_i log(\\hat{y}_i) + (1-y_i)log(1-\\hat{y}_i)$\n",
    "\n",
    "where $y_i$ is the actual label for i-th image and $\\hat{y}_i$ is prediction of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q5hBpmwxmELC"
   },
   "source": [
    "def compute_loss(true_labels, predictions):\n",
    "  loss = #TODO: Pass labels and predictions to crossentropy loss function\n",
    "  return loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-EVN-xCmTb-"
   },
   "source": [
    "One of the most crucial things during training the network is setting the correct parameters. We've already chosen one - batch size. Now let's add a few more:\n",
    "* Learning rate - size of steps we take in gradient descent. It can't be too small, because we will find ourselves in local minimum, but it can't be too big either - we will jump all across the loss function and never converge.\n",
    "* Number of training epochs - how many times we will pass the entire dataset to the model during training.\n",
    "* Optimizer - determines how the network will be updated after each iteration based on loss value, it basically implements some variant of Stochastic Gradient Descent. For now, we will use Adam optimizer - a relatively new invention which is often used with great results.\n",
    "\n",
    "I filled the parameters for you, but don't worry - you will experiment with them later! "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vlOpLAEpo6t_"
   },
   "source": [
    "learning_rate = 5e-3\n",
    "training_epochs = 5\n",
    "optimizer = tf.optimizers.Adam(learning_rate)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3RRTZFRo-GB"
   },
   "source": [
    "We have defined both our model and loss function, so let's move to backpropagating the gradients. First we will create the model and then define a single train step used during the training. At each step, we want to feed the input batch ($x$) to the model, calculate the loss based on actual labels ($y$) and predictions ($\\hat{y}$) and compute the gradients of loss functions with respect to model parameters. The last step will be using our optimizer to adjust weights in the model and return loss value for current step. This task might be the hardest one so far, but when you're done - it will almost be the end!\n",
    "\n",
    "Hints:\n",
    "* `tf.keras.models.Sequential()` is callable, so you can pass the data and get predictions simply by calling it and providing input batch as the argument\n",
    "* You can calculate all gradients at once using a single GradientTape - all model parameters can be found at `model.trainable_variables`\n",
    "* To backpropagate the gradients, use `apply_gradients` method of our optimizer\n",
    "\n",
    "If you're not sure what `tf.function` decorator stands for - it takes care of changing our function into Tensorflow graph. You don't have to worry about it for now."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Vv9Nwdohs9iu"
   },
   "source": [
    "model = create_model()\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    y_hat = #TODO: Use the model to get predictions\n",
    "\n",
    "    loss = #TODO: Calculate loss\n",
    "\n",
    "    gradients = #TODO: Calculate gradients\n",
    "    \n",
    "    #TODO: Backpropagate gradients\n",
    "\n",
    "    return loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKGaaCr-ANyG"
   },
   "source": [
    "Good job! Now it's finally time to train the network. We will take a batch of images and labels from training dataset and pass it to our train_step function. We will also plot loss value for each step. Before running the cell, try to guess the answer - what will happen with loss value during the training?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "nX7rLFNipE9y",
    "outputId": "dfa2b4fa-3219-4e9f-f587-7f962000ab86"
   },
   "source": [
    "history = []\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "  for step, (\"\"\"TODO: Get image and label batch\"\"\") in enumerate(train_dataset):\n",
    "    loss = #TODO: Use our train_step function\n",
    "    history.append(loss.numpy().mean())\n",
    "    plotter.plot(history)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mD9bu95FBOPp"
   },
   "source": [
    "As we can see on the plot - loss value decreases over time. It means that our optimizer is correctly using computed gradients to change weights of the network to make it better and better at recognizing handwritten digits. But now we can ask ourselves - how good this model actually is? We can see that the loss decreases, but that doesn't tell us anything about the accuracy of our predictions. \n",
    "\n",
    "That's where we will use our test set. Earlier in the notebook we made an assumption - test set can't be used during training. This is really important, because it allows us to check how our model performs on data that it has never seen before.\n",
    "\n",
    "As our test metric we will use `tf.keras.metrics.SparseCategoricalAccuracy` - it calculates how often actual labels match our predictions. You just need to pass them as arguments - let's try it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8mxJyRByS2F6",
    "outputId": "cdebc2e9-8863-49c0-fe9e-91c7271ebe07"
   },
   "source": [
    "def evaluate_model(model):\n",
    "    test_metric = #TODO: Pick test metric\n",
    "    test_acc = #TODO: Calculate test accuracy\n",
    "    print(f\"Test accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "evaluate_model(model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Io4Xzy4Cnup"
   },
   "source": [
    "If your accuracy is somewhere over 95% - you did it! You've just built your own neural network with the ability to recognize digits. If you are the type of person who doesn't believe something until you see it - let's visualize our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "3zSgKEf1DO6j",
    "outputId": "2cd0b023-810a-41a9-e838-65cc43f05676"
   },
   "source": [
    "def plot_predictions(predictions_model):\n",
    "  predictions = predictions_model.predict(test_images)\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for i in range(25):\n",
    "      plt.subplot(5, 5, i+1)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      plt.grid(False)\n",
    "      plt.imshow(original_images[\"test\"][i], cmap=plt.cm.binary)\n",
    "      plt.xlabel(np.argmax(predictions[i]))\n",
    "  plt.show()\n",
    "\n",
    "plot_predictions(model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agQCr-eQDdoZ"
   },
   "source": [
    "#4.3 Train the network using model compilation\n",
    "\n",
    "You successfully created your network but there was quite a lot of coding, wasn't it? We used GradientTape() to see how Tensorflow uses it during the training, but there is a quicker way to implement simple models like this - using model compilation. Instead of creating our own loss function, train step etc., we simply provide the most important parameters to `model.compile()`. In our case it will be the name of the optimizer, loss function and metrics (what metrics we want to evaluate during training). "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D1AYJ3CXTwVQ"
   },
   "source": [
    "model = create_model()\n",
    "\n",
    "model.compile(optimizer=\"\"\"TODO: Pick optimizer\"\"\",\n",
    "              loss=\"\"\"TODO: Pick loss function\"\"\",\n",
    "              metrics=[\"accuracy\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoUYO6bqHuIo"
   },
   "source": [
    "Before we feed the data to the model, we have to change our labels a bit - instead of a single label, we need to convert it to a 1D vector, where all values will be 0 except the value at index equal to the label - it will be 1. Such representation of data is also called one-hot encoding.   "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uiW7coCxIGKV",
    "outputId": "3e3d2ecd-6b89-4b60-a634-75b0d99ea6be"
   },
   "source": [
    "print(f\"Label before encoding: {train_labels[0]}\")\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
    "print(f\"Label after encoding: {train_labels[0]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovxInLb1F_xy"
   },
   "source": [
    "After compilation of the model, we can train the network using `model.fit()`, once again providing necessary arguments. Try to find yourself what you need to pass to it!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVk2XWCDGEf9",
    "outputId": "d57bd08a-490c-45a9-c346-98df0a0ee4bc"
   },
   "source": [
    "model.fit(\"\"\"TODO: Pass arguments\"\"\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uer4iEUuGFEh"
   },
   "source": [
    "And finally, to evaluate how our model performs on new data, we will use `model.evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3Q-edlEGWZy",
    "outputId": "754c0e47-cb4a-43ae-b18f-9fe930f6da5d"
   },
   "source": [
    "test_loss, test_acc = model.evaluate(\"\"\"TODO: Pass arguments\"\"\")\n",
    "print(f\"Test accuracy: {test_acc*100:.2f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8K_rR75GXqd"
   },
   "source": [
    "As you could see, using model compilation was much faster than writing the whole training implementation yourself. However, bear in mind that sometimes, especially when dealing with more complex network architectures and algorithms, you might find this approach a bit too simplified and that's when writing your own train_step or loss function might come in handy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEu_bSZJXP1"
   },
   "source": [
    "#5. Experiment!\n",
    "\n",
    "As I said earlier, there are no strict rules in creating neural networks. Nothing limits you, the model will always give you some answers - in the worst case they will just be less accurate than picking them at random. The task for you is to tweak the parameters and see how they affect performance of the model, maybe you will get better accuracy than now? This is also a great moment to explore things that were omitted in this notebook and during the lecture. I would suggest starting with those things:\n",
    "* Change the number of units in the dense layer\n",
    "* Add more layers\n",
    "* Change activation functions\n",
    "* Change optimizers\n",
    "* ... and much more!"
   ]
  }
 ]
}